---
title: "Capstone"
author: "Sergii Sorokolat"
date: "7/15/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Define a function for reading from a file and sampling a small portion of the data

```{r}
readAndSampleFile <- function(fileName, sampleSize ) {
    
    con <- file(fileName, open = "r", encoding = "UTF-8")
    lines <- readLines(con)
    
    idxs <- sample(1:length(lines), sampleSize)
    vals <- c()
    
     for (i in 1:length(lines)) {
        if (i %in% idxs) {
           vals <- append(vals, lines[i])
        }
     }
    close(con)
    vals
}
```

```{r}
removeBadWords <- function(t, pattern) {
  for (i in 1:nrow(t)) {
    if (t[i, 2] == TRUE) { ## sanity failed
      t[i,1] <- gsub(pattern, "*", t[i,1])
    }
  }
  t
}
```

## read and sample

```{r warning=FALSE, message=FALSE}
library(tidyverse)
setwd("~/Desktop/dscapstone/")
##setwd("~/GitHub/dscapstone/")

set.seed(7152018)

## sample size
s <- 50000
## samples <- c()

## samples <- append(samples, readAndSampleFile(fileName = "final/en_US/en_US.blogs.txt", s))
## samples <- append(samples, readAndSampleFile(fileName = "final/en_US/en_US.news.txt", s))
## samples <- append(samples, readAndSampleFile(fileName = "final/en_US/en_US.twitter.txt", s))

## cleanup
## gc()

## saveRDS(samples, "samples150k.rds")

samples <- readRDS("samples150k.rds")
## summary(samples)
```

## Profanity filtering
https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words

```{r message=FALSE, eval=FALSE}

 filter <- read.csv("sanityfilter_en.txt", header = FALSE, sep = "\n", as.is = TRUE)$V1

## to lowercase
t <- tbl_df(samples)
t <- mutate(t, value = str_to_lower(value))

## build the grep filter
f1 <- c()
for (i in 1:length(filter)) {
  ## \\<word\\>
  word <- paste("\\<", filter[i] ,"\\>", sep = "")
  f1 <- append(f1, word)
}
f <- paste(f1, sep = "", collapse = "|")
t$sanity_failed <- grepl(paste("(", f, ")", sep = ""), t$value)
# ##t$value <- gsub(paste("(", f, ")", sep = ""), "0", t$value, fixed = TRUE)

##t <-readRDS("samples150k-sanityfilter.rds")
# nrow(t[t$sanity_failed,])
#
saveRDS(t, "filtered.rds")



```

```{r eval=FALSE}
samplesFiltered <- readRDS("filtered.rds")
t <- tbl_df(samplesFiltered)
nrow(t[t$sanity_failed,])

filter <- read.csv("sanityfilter_en.txt", header = FALSE, sep = "\n", as.is = TRUE)$V1
f1 <- c()
for (i in 1:length(filter)) {
  ## \\<word\\>
  word <- paste("\\<", filter[i] ,"\\>", sep = "")
  f1 <- append(f1, word)
}
f <- paste(f1, sep = "", collapse = "|")

t <- removeBadWords(t, paste("(", f, ")", sep = ""))
saveRDS(t, "preprocessed_needs_rm_spec_chars.rds")
```


```{r message=FALSE, warning=FALSE, cache=TRUE}
library(tidyverse)
library(tm)
library(RWeka)

t <- tbl_df(readRDS("preprocessed_needs_rm_spec_chars.rds"))
nrow(t[t$sanity_failed,])

corp <- VCorpus(VectorSource(t$value))

corp <- tm_map(corp, content_transformer(function(x, pattern) gsub(pattern, " ", x)),  "“|”|–|…|‘|’")
corp <- tm_map(corp, content_transformer(removePunctuation))
corp <- tm_map(corp, content_transformer(removeNumbers))

corp <- tm_map(corp, stripWhitespace)

ngramtoken1 <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
ngramtoken2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngramtoken3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

gc()

ngrammatr1 <- TermDocumentMatrix(corp, control = list(tokenize = ngramtoken1))
ngrammatr2 <- TermDocumentMatrix(corp, control = list(tokenize = ngramtoken2))
ngrammatr3 <- TermDocumentMatrix(corp, control = list(tokenize = ngramtoken3))


gc()

```


## Unigrams
```{r}
library(tm)
library(RWeka)
freqTerms <- findFreqTerms(ngrammatr1, lowfreq = 6400)
termFrequency <- rowSums(as.matrix(ngrammatr1[freqTerms,]))
termFrequency <- data.frame(ngrm=names(termFrequency), frequency=termFrequency)

g <- ggplot(termFrequency, aes(x=reorder(ngrm, frequency), y=frequency)) +
    geom_bar(stat = "identity") +  coord_flip() +
    theme(legend.title=element_blank()) +
    xlab("Ngram") + ylab("Frequency") +
    labs(title = "Top ngrams by frequency")
print(g)
```


## Bigrams

```{r}
library(tm)
library(RWeka)
freqTerms <- findFreqTerms(ngrammatr2, lowfreq = 2400)
termFrequency <- rowSums(as.matrix(ngrammatr2[freqTerms,]))
termFrequency <- data.frame(ngrm=names(termFrequency), frequency=termFrequency)

g <- ggplot(termFrequency, aes(x=reorder(ngrm, frequency), y=frequency)) +
    geom_bar(stat = "identity") +  coord_flip() +
    theme(legend.title=element_blank()) +
    xlab("Ngram") + ylab("Frequency") +
    labs(title = "Top ngrams by frequency")
print(g)
```


## Trigrams

```{r}
library(tm)
library(RWeka)
freqTerms <- findFreqTerms(ngrammatr3, lowfreq = 350)
termFrequency <- rowSums(as.matrix(ngrammatr3[freqTerms,]))
termFrequency <- data.frame(ngrm=names(termFrequency), frequency=termFrequency)

g <- ggplot(termFrequency, aes(x=reorder(ngrm, frequency), y=frequency)) +
    geom_bar(stat = "identity") +  coord_flip() +
    theme(legend.title=element_blank()) +
    xlab("Ngram") + ylab("Frequency") +
    labs(title = "Top ngrams by frequency")
print(g)
```




